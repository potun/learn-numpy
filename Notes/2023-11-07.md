# Singular Value Decomposition (SVD)
$$A=U \Sigma V^T$$
Singular Value Decomposition (SVD) is a fundamental matrix factorization technique used in linear algebra, numerical analysis, and various applications. It decomposes a matrix into three other matrices, revealing the essential characteristics of the original matrix. The basic idea behind SVD is to represent a matrix as a product of three simpler matrices:

1. **The Original Matrix ($A$):** The matrix you want to decompose. It can be of any shape but is typically a rectangular or square matrix.

2. **Left Singular Vectors ($U$):** $U$ is a matrix containing the left singular vectors. These vectors are orthogonal and represent the relationships between rows in the original matrix. The columns of $U$ form an orthonormal basis for the column space of $A$.

3. **Singular Values ($\Sigma$):** $\Sigma$ is a diagonal matrix containing the singular values of the original matrix. Singular values are positive real numbers and represent the scaling factors for the singular vectors in $U$ and $V$.

4. **Right Singular Vectors ($V^T$):** $V$ is a matrix containing the right singular vectors. These vectors are orthogonal and represent the relationships between columns in the original matrix. $V^T$ (or $V^H$ for complex matrices) is the transpose or conjugate transpose of $V$.

Mathematically, the SVD of a matrix $A$ is represented as:

$$A=U \Sigma V^T$$

Here's a step-by-step breakdown of the SVD process:

1. Calculate the product of $A$ and its transpose $A^T$ (for real matrices) or its conjugate transpose $A^H$ (for complex matrices).

2. Find the eigenvalues and eigenvectors of the matrix $A^T A$ (or $A^H A$ for complex) to get the singular values and right singular vectors. The singular values are the square roots of the eigenvalues.

3. Normalize the right singular vectors to obtain the matrix $V$.

4. Find the left singular vectors by calculating $U = A  V  \Sigma^{-1}$ (where $\Sigma^{-1}$ is the inverse of the diagonal matrix $\Sigma$).

SVD has numerous applications, including dimensionality reduction, data compression, matrix approximation, image compression, and solving linear systems of equations. It is a powerful tool for extracting and representing the most important information in a matrix while reducing noise and redundancy.

One application of SVD in machine learning is matrix completion. In scenarios where data is missing or sparse, SVD can be used for matrix completion. By filling in missing entries in a matrix based on the low-rank approximation (**LoRA**) obtained through SVD, you can impute missing data and make predictions.